"""
Search Node - Finds educational resources using Tavily and YouTube APIs

Production-ready implementation with:
- Tavily API for web article search
- YouTube Data API v3 for video search  
- Retry logic with exponential backoff
- Smart query optimization from gap analysis
- Parallel execution for speed
- Quality filtering and relevance scoring
"""

import logging
import os
import asyncio
import re
from typing import Dict, Any, List

_logger = logging.getLogger(__name__)


async def search_resources(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    Search for educational resources using Tavily (articles) and YouTube (videos).
    
    Uses search queries generated by gap analysis for optimal results.
    Searches are executed in parallel for performance.
    
    Args:
        state: Current workflow state with:
            - search_queries: List of optimized search queries from gap analysis
            - keywords: Fallback if search_queries not available
            - preferences: Search preferences (limits, filters)
        
    Returns:
        Updated state with:
            - articles: List of article resources from Tavily
            - videos: List of video resources from YouTube
            - error: Error message if search failed
    
    Example:
        >>> state = {
        ...     "search_queries": ["python basics tutorial", "variables in python"],
        ...     "preferences": {"resource_limit": 10}
        ... }
        >>> result = await search_resources(state)
        >>> print(len(result["articles"]), len(result["videos"]))
        20 20
    """
    _logger.info("üîé Starting resource search with Tavily + YouTube...")
    
    try:
        # Get search queries (prefer gap analysis queries over raw keywords)
        search_queries = state.get("search_queries", [])
        keywords = state.get("keywords", [])
        preferences = state.get("preferences", {})
        
        # Use search_queries if available, otherwise fallback to keywords
        queries = search_queries if search_queries else keywords
        
        if not queries:
            _logger.warning("‚ö†Ô∏è No search queries or keywords available, skipping search")
            state["articles"] = []
            state["videos"] = []
            return state
        
        _logger.info(f"üìù Using {len(queries)} search queries")
        
        # Search articles and videos in parallel for speed
        articles_task = _search_articles(queries, preferences)
        videos_task = _search_videos(queries, preferences)
        
        articles, videos = await asyncio.gather(articles_task, videos_task)
        
        state["articles"] = articles
        state["videos"] = videos
        
        _logger.info(
            f"‚úÖ Search complete: {len(articles)} articles, {len(videos)} videos found"
        )
        return state
        
    except Exception as e:
        _logger.error(f"‚ùå Resource search failed: {type(e).__name__}: {e}")
        state["error"] = f"Search error: {str(e)}"
        state["articles"] = []
        state["videos"] = []
        return state


async def _search_articles(queries: List[str], preferences: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Search for educational articles using Tavily API.
    
    Features:
    - Uses search queries from gap analysis
    - Retry logic for network failures
    - Quality filtering (educational content)
    - Relevance scoring
    
    Args:
        queries: List of search queries (from gap analysis or keywords)
        preferences: Search preferences with resource_limit, include_articles
    
    Returns:
        List of article resources with title, url, description, source, relevance_score
    """
    if not preferences.get("include_articles", True):
        return []
    
    # Get more candidates than needed for aggregation to rank
    limit = preferences.get("resource_limit", 10) * 2
    
    return await _search_with_tavily(queries, limit)


async def _search_videos(queries: List[str], preferences: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Search for educational videos using YouTube Data API v3.
    
    Features:
    - Uses search queries from gap analysis
    - Retry logic for network failures  
    - Education category filtering
    - Duration and metadata extraction
    
    Args:
        queries: List of search queries (from gap analysis or keywords)
        preferences: Search preferences with resource_limit, include_videos
    
    Returns:
        List of video resources with title, url, thumbnail, channel, duration, source, relevance_score
    """
    if not preferences.get("include_videos", True):
        return []
    
    # Get more candidates than needed for aggregation to rank
    limit = preferences.get("resource_limit", 10) * 2
    
    return await _search_with_youtube(queries, limit)



async def _search_with_tavily(queries: List[str], limit: int) -> List[Dict[str, Any]]:
    """
    Search educational articles using Tavily API with retry logic.
    
    Uses multiple search queries and deduplicates results.
    Optimized for educational content with "tutorial", "guide", "learning" terms.
    
    Features:
    - Retry logic (3 attempts with exponential backoff)
    - Multiple query execution
    - Educational content optimization
    - Relevance scoring
    - Deduplication by URL
    
    Args:
        queries: List of search queries
        limit: Maximum number of articles to return
    
    Returns:
        List of article dictionaries with title, url, description, source, relevance_score
    
    Raises:
        ValueError: If TAVILY_API_KEY not set
        Exception: If search fails after all retries
    
    Requires:
        TAVILY_API_KEY environment variable
    """
    max_retries = 3
    retry_delay = 2  # seconds
    
    try:
        # Get API key
        api_key = os.getenv("TAVILY_API_KEY")
        if not api_key:
            raise ValueError(
                "TAVILY_API_KEY not found. Set TAVILY_API_KEY environment variable."
            )
        
        for attempt in range(max_retries):
            try:
                from tavily import TavilyClient
                
                if attempt > 0:
                    _logger.info(f"üîÑ Retry attempt {attempt + 1}/{max_retries}")
                
                _logger.info(f"üåê Searching articles with Tavily ({len(queries)} queries)...")
                
                # Initialize client
                client = TavilyClient(api_key=api_key)
                
                # Combine top queries for comprehensive search
                # Use top 3 queries to avoid quota issues
                top_queries = queries[:3]
                
                all_articles = []
                seen_urls = set()
                
                for query in top_queries:
                    # Optimize query for educational content
                    educational_query = f"{query} tutorial guide learning"
                    
                    _logger.info(f"  üìù Query: {educational_query[:60]}...")
                    
                    # Execute search (blocking call, run in executor)
                    loop = asyncio.get_event_loop()
                    response = await loop.run_in_executor(
                        None,
                        lambda: client.search(
                            query=educational_query,
                            max_results=limit // len(top_queries) + 2,  # Distribute limit
                            search_depth="advanced",
                            include_domains=[],  # No restrictions
                            exclude_domains=["pinterest.com", "facebook.com"]  # Skip social
                        )
                    )
                    
                    # Extract results
                    for result in response.get("results", []):
                        url = result.get("url", "")
                        
                        # Deduplicate by URL
                        if url and url not in seen_urls:
                            seen_urls.add(url)
                            
                            all_articles.append({
                                "title": result.get("title", "Untitled"),
                                "url": url,
                                "description": result.get("content", "")[:500],  # Limit length
                                "source": "web_search",
                                "relevance_score": float(result.get("score", 0.5))
                            })
                
                # Sort by relevance and limit
                all_articles.sort(key=lambda x: x["relevance_score"], reverse=True)
                final_articles = all_articles[:limit]
                
                _logger.info(
                    f"‚úÖ Tavily search successful: {len(final_articles)} unique articles "
                    f"(from {len(all_articles)} total results)"
                )
                
                return final_articles
                
            except ImportError:
                _logger.error("‚ùå Tavily SDK not installed")
                raise ValueError(
                    "Tavily SDK not installed. Install with: pip install tavily-python"
                )
                
            except (ConnectionError, TimeoutError, OSError) as e:
                # Network-related errors - retry with exponential backoff
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (2 ** attempt)
                    _logger.warning(f"‚ö†Ô∏è Network error: {e}. Retrying in {wait_time}s...")
                    await asyncio.sleep(wait_time)
                    continue
                else:
                    _logger.error(f"‚ùå Tavily search failed after {max_retries} attempts: {e}")
                    raise
                    
            except Exception as e:
                # Other errors - log and retry
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (2 ** attempt)
                    _logger.warning(
                        f"‚ö†Ô∏è Tavily error: {type(e).__name__}: {e}. Retrying in {wait_time}s..."
                    )
                    await asyncio.sleep(wait_time)
                    continue
                else:
                    _logger.error(f"‚ùå Tavily search failed: {type(e).__name__}: {e}")
                    raise
        
        # Should never reach here
        raise Exception(f"Tavily search failed after {max_retries} retry attempts")
        
    except Exception as e:
        _logger.error(f"‚ùå Fatal error in Tavily search: {e}")
        raise


async def _search_with_youtube(queries: List[str], limit: int) -> List[Dict[str, Any]]:
    """
    Search educational videos using YouTube Data API v3 with retry logic.
    
    Uses multiple search queries, filters for education category,
    and extracts video metadata (duration, thumbnail, channel).
    
    Features:
    - Retry logic (3 attempts with exponential backoff)
    - Multiple query execution
    - Education category filtering
    - Duration extraction (requires additional API call)
    - Deduplication by video ID
    
    Args:
        queries: List of search queries
        limit: Maximum number of videos to return
    
    Returns:
        List of video dictionaries with title, url, thumbnail, channel, duration, source, relevance_score
    
    Raises:
        ValueError: If YOUTUBE_API_KEY not set
        Exception: If search fails after all retries
    
    Requires:
        YOUTUBE_API_KEY environment variable
    """
    max_retries = 3
    retry_delay = 2  # seconds
    
    try:
        # Get API key
        api_key = os.getenv("YOUTUBE_API_KEY")
        if not api_key:
            raise ValueError(
                "YOUTUBE_API_KEY not found. Set YOUTUBE_API_KEY environment variable."
            )
        
        for attempt in range(max_retries):
            try:
                from googleapiclient.discovery import build
                
                if attempt > 0:
                    _logger.info(f"üîÑ Retry attempt {attempt + 1}/{max_retries}")
                
                _logger.info(f"üì∫ Searching videos with YouTube API ({len(queries)} queries)...")
                
                # Build YouTube client (blocking call, run in executor)
                loop = asyncio.get_event_loop()
                youtube = await loop.run_in_executor(
                    None,
                    lambda: build('youtube', 'v3', developerKey=api_key)
                )
                
                # Combine top queries
                top_queries = queries[:3]  # Use top 3 to manage quota
                
                all_videos = []
                seen_video_ids = set()
                
                for query in top_queries:
                    # Optimize query for educational content
                    educational_query = f"{query} tutorial course explained"
                    
                    _logger.info(f"  üìù Query: {educational_query[:60]}...")
                    
                    # Execute search (blocking call, run in executor)
                    search_response = await loop.run_in_executor(
                        None,
                        lambda q=educational_query: youtube.search().list(
                            q=q,
                            part='id,snippet',
                            maxResults=limit // len(top_queries) + 2,  # Distribute limit
                            type='video',
                            videoCategoryId='27',  # Education category
                            relevanceLanguage='en',
                            safeSearch='moderate'
                        ).execute()
                    )
                    
                    # Extract video IDs for duration lookup
                    video_ids = []
                    video_data = {}
                    
                    for item in search_response.get('items', []):
                        video_id = item['id']['videoId']
                        
                        if video_id not in seen_video_ids:
                            seen_video_ids.add(video_id)
                            video_ids.append(video_id)
                            video_data[video_id] = item['snippet']
                    
                    # Get video details (duration, etc.) in batch
                    if video_ids:
                        video_details_response = await loop.run_in_executor(
                            None,
                            lambda: youtube.videos().list(
                                part='contentDetails,statistics',
                                id=','.join(video_ids)
                            ).execute()
                        )
                        
                        for item in video_details_response.get('items', []):
                            video_id = item['id']
                            snippet = video_data[video_id]
                            content_details = item['contentDetails']
                            
                            # Parse ISO 8601 duration (PT1H2M3S)
                            duration_iso = content_details.get('duration', 'PT0S')
                            duration_readable = _parse_youtube_duration(duration_iso)
                            
                            all_videos.append({
                                "title": snippet['title'],
                                "url": f"https://www.youtube.com/watch?v={video_id}",
                                "thumbnail": snippet['thumbnails'].get('high', {}).get('url', ''),
                                "channel": snippet['channelTitle'],
                                "duration": duration_readable,
                                "source": "video_platform",
                                "relevance_score": 0.85  # YouTube doesn't provide relevance scores
                            })
                
                # Limit results
                final_videos = all_videos[:limit]
                
                _logger.info(
                    f"‚úÖ YouTube search successful: {len(final_videos)} unique videos "
                    f"(from {len(all_videos)} total results)"
                )
                
                return final_videos
                
            except ImportError:
                _logger.error("‚ùå Google API client not installed")
                raise ValueError(
                    "Google API client not installed. Install with: pip install google-api-python-client"
                )
                
            except (ConnectionError, TimeoutError, OSError) as e:
                # Network-related errors - retry with exponential backoff
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (2 ** attempt)
                    _logger.warning(f"‚ö†Ô∏è Network error: {e}. Retrying in {wait_time}s...")
                    await asyncio.sleep(wait_time)
                    continue
                else:
                    _logger.error(f"‚ùå YouTube search failed after {max_retries} attempts: {e}")
                    raise
                    
            except Exception as e:
                # Other errors - log and retry
                if attempt < max_retries - 1:
                    wait_time = retry_delay * (2 ** attempt)
                    _logger.warning(
                        f"‚ö†Ô∏è YouTube error: {type(e).__name__}: {e}. Retrying in {wait_time}s..."
                    )
                    await asyncio.sleep(wait_time)
                    continue
                else:
                    _logger.error(f"‚ùå YouTube search failed: {type(e).__name__}: {e}")
                    raise
        
        # Should never reach here
        raise Exception(f"YouTube search failed after {max_retries} retry attempts")
        
    except Exception as e:
        _logger.error(f"‚ùå Fatal error in YouTube search: {e}")
        raise


def _parse_youtube_duration(duration_iso: str) -> str:
    """
    Parse ISO 8601 duration (PT1H2M3S) to readable format (1:02:03).
    
    Args:
        duration_iso: ISO 8601 duration string (e.g., "PT1H2M3S", "PT15M30S", "PT45S")
    
    Returns:
        Human-readable duration (e.g., "1:02:03", "15:30", "0:45")
    
    Examples:
        >>> _parse_youtube_duration("PT1H2M3S")
        "1:02:03"
        >>> _parse_youtube_duration("PT15M30S")
        "15:30"
        >>> _parse_youtube_duration("PT45S")
        "0:45"
    """
    # Extract hours, minutes, seconds using regex
    hours = re.search(r'(\d+)H', duration_iso)
    minutes = re.search(r'(\d+)M', duration_iso)
    seconds = re.search(r'(\d+)S', duration_iso)
    
    h = int(hours.group(1)) if hours else 0
    m = int(minutes.group(1)) if minutes else 0
    s = int(seconds.group(1)) if seconds else 0
    
    # Format as H:MM:SS or M:SS or 0:SS
    if h > 0:
        return f"{h}:{m:02d}:{s:02d}"
    else:
        return f"{m}:{s:02d}"
