"""
Lecture Insight Agent - FastAPI Application
Worker agent that processes lecture audio and returns educational resources.
"""

import logging
import uuid
import time
from fastapi import FastAPI, Request, HTTPException
from contextlib import asynccontextmanager
from datetime import datetime

from shared.models import TaskEnvelope, CompletionReport
from agents.lecture_insight.models import LectureInsightInput, LectureInsightOutput
from agents.lecture_insight import ltm

logging.basicConfig(level=logging.INFO)
_logger = logging.getLogger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Initialize resources on startup, cleanup on shutdown."""
    _logger.info("üéì Lecture Insight Agent starting up...")
    await ltm.init_db()
    # TODO: Initialize LangGraph workflow
    yield
    _logger.info("üéì Lecture Insight Agent shutting down...")


app = FastAPI(
    title="Lecture Insight Agent",
    description="Processes lecture audio and returns curated educational resources",
    version="1.0.0",
    lifespan=lifespan
)


@app.get('/health')
async def health():
    """Health check endpoint."""
    return {
        "status": "healthy",
        "agent": "lecture-insight",
        "version": "1.0.0",
        "timestamp": datetime.utcnow().isoformat()
    }


@app.get('/agent-info')
async def agent_info():
    """Agent capabilities and information."""
    return {
        "agent_id": "lecture-insight",
        "agent_name": "Lecture Insight Agent",
        "description": "Processes lecture audio and returns curated learning materials",
        "capabilities": [
            "audio-transcription",
            "content-summarization",
            "keyword-extraction",
            "educational-resource-search"
        ],
        "supported_formats": ["wav", "mp3", "m4a"],
        "version": "1.0.0"
    }


@app.get('/cache-stats')
async def cache_stats():
    """Get LTM cache statistics."""
    stats = await ltm.get_stats()
    return {
        "cache": stats,
        "timestamp": datetime.utcnow().isoformat()
    }


@app.post('/process', response_model=CompletionReport)
async def process_task(req: Request):
    """
    Main processing endpoint. Receives TaskEnvelope from supervisor,
    processes lecture audio, returns CompletionReport with results.
    """
    start_time = time.time()
    
    # Parse incoming request
    try:
        body = await req.json()
        task_envelope = TaskEnvelope(**body)
    except Exception as e:
        _logger.error(f"Invalid request body: {e}")
        raise HTTPException(status_code=400, detail=f"Invalid request body: {e}")
    
    _logger.info(f"üì• Received task: {task_envelope.message_id}")
    
    # Extract and validate input parameters
    try:
        input_data = LectureInsightInput(**task_envelope.task.parameters)
    except Exception as e:
        _logger.error(f"Invalid task parameters: {e}")
        return CompletionReport(
            message_id=str(uuid.uuid4()),
            sender="LectureInsightAgent",
            recipient=task_envelope.sender,
            related_message_id=task_envelope.message_id,
            status="FAILURE",
            results={"error": f"Invalid parameters: {str(e)}"}
        )
    
    # Check LTM cache first
    cached_output = await ltm.lookup(
        audio_data=input_data.audio_input.data,
        preferences=input_data.preferences.model_dump()
    )
    if cached_output:
        _logger.info(f"‚ö° Returning cached result for session {input_data.session_id}")
        return CompletionReport(
            message_id=str(uuid.uuid4()),
            sender="LectureInsightAgent",
            recipient=task_envelope.sender,
            related_message_id=task_envelope.message_id,
            status="SUCCESS",
            results={"output": cached_output, "cached": True}
        )
    
    # TODO: Process through LangGraph workflow (Phase 2.4-2.5)
    # For now, return a placeholder response
    try:
        # Placeholder processing
        output = {
            "session_id": input_data.session_id,
            "transcript": "[Placeholder] Transcript will be generated by transcription node",
            "summary": "[Placeholder] Summary will be generated by summarization node",
            "keywords": ["placeholder", "keywords"],
            "learning_gaps": [],
            "resources": {
                "articles": [],
                "videos": []
            },
            "metadata": {
                "processing_time_seconds": time.time() - start_time,
                "audio_duration_seconds": None,
                "timestamp": datetime.utcnow().isoformat()
            }
        }
        
        # Save to LTM cache for future requests
        await ltm.save(
            audio_data=input_data.audio_input.data,
            preferences=input_data.preferences.model_dump(),
            output=output
        )
        
        _logger.info(f"‚úÖ Task completed: {task_envelope.message_id}")
        
        return CompletionReport(
            message_id=str(uuid.uuid4()),
            sender="LectureInsightAgent",
            recipient=task_envelope.sender,
            related_message_id=task_envelope.message_id,
            status="SUCCESS",
            results={"output": output, "cached": False}
        )
        
    except Exception as e:
        _logger.exception(f"‚ùå Task failed: {e}")
        return CompletionReport(
            message_id=str(uuid.uuid4()),
            sender="LectureInsightAgent",
            recipient=task_envelope.sender,
            related_message_id=task_envelope.message_id,
            status="FAILURE",
            results={"error": str(e)}
        )
